{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.11.2)\n",
      "Requirement already satisfied: numexpr in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: funcy in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: sklearn in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: scipy in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.5.2)\n",
      "Requirement already satisfied: gensim in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.3.3)\n",
      "Requirement already satisfied: future in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.23.2)\n",
      "Requirement already satisfied: setuptools in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (50.3.1.post20201107)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.21.2)\n",
      "Requirement already satisfied: joblib in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2020.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/megankane/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve and Convert File to Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Book + Chunk</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0 Chunk 0</td>\n",
       "      <td>1 And Ardrahan Ardrahan As As At By CHAPTER En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0 Chunk 1</td>\n",
       "      <td>Afterwards And And And And And Anyway As Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0 Chunk 2</td>\n",
       "      <td>And And Arriving As As But Come Curiously For ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0 Chunk 3</td>\n",
       "      <td>2 A A All And And As Borderland Bps CHAPTER Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0 Chunk 4</td>\n",
       "      <td>After An As Away Down For Further Gradually Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8134</th>\n",
       "      <td>8134</td>\n",
       "      <td>0 Chunk 8134</td>\n",
       "      <td>Acentinol Agent Agent All And Below Below But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8135</th>\n",
       "      <td>8135</td>\n",
       "      <td>0 Chunk 8135</td>\n",
       "      <td>1 A Acentinol Agent All All And Archive Archiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8136</th>\n",
       "      <td>8136</td>\n",
       "      <td>0 Chunk 8136</td>\n",
       "      <td>A A ARE Acres Ahhh All And And Archive Archive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8137</th>\n",
       "      <td>8137</td>\n",
       "      <td>0 Chunk 8137</td>\n",
       "      <td>A All All And And Anytime Archive Archive Arch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8138</th>\n",
       "      <td>8138</td>\n",
       "      <td>0 Chunk 8138</td>\n",
       "      <td>And And And And Before Carrie Carrie Carrie Fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8139 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Book + Chunk  \\\n",
       "0              0     0 Chunk 0   \n",
       "1              1     0 Chunk 1   \n",
       "2              2     0 Chunk 2   \n",
       "3              3     0 Chunk 3   \n",
       "4              4     0 Chunk 4   \n",
       "...          ...           ...   \n",
       "8134        8134  0 Chunk 8134   \n",
       "8135        8135  0 Chunk 8135   \n",
       "8136        8136  0 Chunk 8136   \n",
       "8137        8137  0 Chunk 8137   \n",
       "8138        8138  0 Chunk 8138   \n",
       "\n",
       "                                                   Text  \n",
       "0     1 And Ardrahan Ardrahan As As At By CHAPTER En...  \n",
       "1     Afterwards And And And And And Anyway As Down ...  \n",
       "2     And And Arriving As As But Come Curiously For ...  \n",
       "3     2 A A All And And As Borderland Bps CHAPTER Cr...  \n",
       "4     After An As Away Down For Further Gradually Ha...  \n",
       "...                                                 ...  \n",
       "8134  Acentinol Agent Agent All And Below Below But ...  \n",
       "8135  1 A Acentinol Agent All All And Archive Archiv...  \n",
       "8136  A A ARE Acres Ahhh All And And Archive Archive...  \n",
       "8137  A All All And And Anytime Archive Archive Arch...  \n",
       "8138  And And And And Before Carrie Carrie Carrie Fr...  \n",
       "\n",
       "[8139 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = 'Desktop/SciFi_Texts/'\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('sci-fi_word_chunks_bag_of_words_output (1).csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Book + Chunk</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0 Chunk 0</td>\n",
       "      <td>1 And Ardrahan Ardrahan As As At By CHAPTER En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0 Chunk 1</td>\n",
       "      <td>Afterwards And And And And And Anyway As Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0 Chunk 2</td>\n",
       "      <td>And And Arriving As As But Come Curiously For ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0 Chunk 3</td>\n",
       "      <td>2 A A All And And As Borderland Bps CHAPTER Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0 Chunk 4</td>\n",
       "      <td>After An As Away Down For Further Gradually Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8134</th>\n",
       "      <td>8134</td>\n",
       "      <td>0 Chunk 8134</td>\n",
       "      <td>Acentinol Agent Agent All And Below Below But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8135</th>\n",
       "      <td>8135</td>\n",
       "      <td>0 Chunk 8135</td>\n",
       "      <td>1 A Acentinol Agent All All And Archive Archiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8136</th>\n",
       "      <td>8136</td>\n",
       "      <td>0 Chunk 8136</td>\n",
       "      <td>A A ARE Acres Ahhh All And And Archive Archive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8137</th>\n",
       "      <td>8137</td>\n",
       "      <td>0 Chunk 8137</td>\n",
       "      <td>A All All And And Anytime Archive Archive Arch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8138</th>\n",
       "      <td>8138</td>\n",
       "      <td>0 Chunk 8138</td>\n",
       "      <td>And And And And Before Carrie Carrie Carrie Fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8139 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Book + Chunk  \\\n",
       "0              0     0 Chunk 0   \n",
       "1              1     0 Chunk 1   \n",
       "2              2     0 Chunk 2   \n",
       "3              3     0 Chunk 3   \n",
       "4              4     0 Chunk 4   \n",
       "...          ...           ...   \n",
       "8134        8134  0 Chunk 8134   \n",
       "8135        8135  0 Chunk 8135   \n",
       "8136        8136  0 Chunk 8136   \n",
       "8137        8137  0 Chunk 8137   \n",
       "8138        8138  0 Chunk 8138   \n",
       "\n",
       "                                                   Text  \n",
       "0     1 And Ardrahan Ardrahan As As At By CHAPTER En...  \n",
       "1     Afterwards And And And And And Anyway As Down ...  \n",
       "2     And And Arriving As As But Come Curiously For ...  \n",
       "3     2 A A All And And As Borderland Bps CHAPTER Cr...  \n",
       "4     After An As Away Down For Further Gradually Ha...  \n",
       "...                                                 ...  \n",
       "8134  Acentinol Agent Agent All And Below Below But ...  \n",
       "8135  1 A Acentinol Agent All All And Archive Archiv...  \n",
       "8136  A A ARE Acres Ahhh All And And Archive Archive...  \n",
       "8137  A All All And And Anytime Archive Archive Arch...  \n",
       "8138  And And And And Before Carrie Carrie Carrie Fr...  \n",
       "\n",
       "[8139 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop any empty cells\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add values in Text column to new list (for further cleaning)\n",
    "data = df.Text.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Add further stopwords by simply \"appending\" desired words to dictionary\n",
    "stop_words.append('CHAPTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:3: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:3: DeprecationWarning: invalid escape sequence \\s\n",
      "/var/folders/8k/6tq9yjnx511d5q4fkv00y_600000gn/T/ipykernel_9382/3478888187.py:2: DeprecationWarning: invalid escape sequence \\S\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
      "/var/folders/8k/6tq9yjnx511d5q4fkv00y_600000gn/T/ipykernel_9382/3478888187.py:3: DeprecationWarning: invalid escape sequence \\s\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
     ]
    }
   ],
   "source": [
    "#Remove punctuation\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to perform simple preprocessing on text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run processing function on texts\n",
    "data_words = list(sent_to_words(data))\n",
    "#print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define stopword removal\n",
    "def remove_stopwords(texts):\n",
    "   return [[word for word in simple_preprocess(str(doc))\n",
    "if word not in stop_words] for doc in texts]\n",
    "\n",
    "#Define function to make bigrams\n",
    "def make_bigrams(texts):\n",
    "   return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "#def make_trigrams(texts):\n",
    "#   return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "#Define function to lemmatize texts\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "   texts_out = []\n",
    "   for sent in texts:\n",
    "     doc = nlp(\" \".join(sent))\n",
    "     texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "   return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=1, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chapter', 'find', 'far', 'fuel', 'however', 'ireland_irish', 'irish', 'kraighten', 'kraighten', 'later', 'manuscript', 'possibly', 'right', 'tonnison', 'yet', 'abound', 'add_addresse', 'address', 'aid', 'alone', 'angler', 'appetite', 'around', 'arrive', 'ask', 'ask', 'assent', 'average_aware', 'away', 'away', 'awhile', 'back', 'bacon', 'bad', 'bare', 'base', 'begin', 'betray', 'blanket', 'bleak_blessing', 'bring', 'busy', 'called_calle', 'camp', 'car', 'cast', 'casually', 'chance_chance', 'clinging_close', 'close', 'colony', 'come', 'come', 'come', 'comer', 'come', 'comprehend', 'comrade', 'consult', 'contact', 'cottage', 'country', 'course', 'cover', 'crowd_crowd', 'curiously', 'cut', 'day', 'die', 'direction', 'discover', 'dismiss', 'doorway', 'driver_driver', 'driver', 'early', 'earth', 'elbow', 'elect', 'end', 'entirely', 'erect', 'escape', 'even', 'evening', 'ever', 'evident', 'exist', 'expression', 'eyed', 'face', 'faced_face', 'fact', 'fact', 'family', 'fall', 'fellow_fellow', 'fellow', 'fill', 'fine', 'follow', 'food', 'fortnight', 'friend_friend', 'friendly', 'fryingpan', 'full', 'gaunt', 'get', 'glance', 'good', 'good', 'get', 'great', 'great', 'ground', 'group', 'grunt', 'guessed_guessed', 'guide', 'hamlet', 'head', 'head', 'hear', 'hill', 'hire', 'hitherto', 'hurry', 'hut', 'idea', 'imaginable', 'impartially', 'include', 'indeed_indicate', 'inhabitant', 'inhospitable', 'instead', 'instead', 'isolate', 'jabber', 'jaunt', 'joke_journey', 'know', 'know', 'know', 'land', 'language', 'last', 'lay', 'least', 'leave', 'leave', 'leave', 'lies_lie', 'light', 'little', 'little', 'little', 'little', 'live', 'lodging', 'long', 'long', 'look', 'low', 'make', 'make', 'man', 'man', 'manner_manner', 'many', 'map', 'meal', 'mere', 'might', 'mile', 'minute', 'moment', 'morning', 'morrow', 'name', 'near_nearest', 'need', 'never', 'night', 'nod_nod', 'nod', 'none', 'numerous', 'observation', 'often', 'oilstove', 'outfit', 'outside', 'outskirt', 'overhead', 'part', 'pass', 'past', 'patch', 'peat', 'people', 'people', 'people', 'perhaps', 'pigsty', 'place', 'place', 'place', 'plan', 'possibility', 'prepare', 'provision', 'pure', 'put', 'put', 'puzzlement', 'quite', 'ragged', 'railwaystation', 'rapidly', 'reach', 'receive', 'reflect', 'rest', 'result', 'return_returned', 'ridge', 'rise', 'rock', 'roll', 'roost', 'rough', 'round', 'ruin', 'run', 'say', 'say', 'say', 'say', 'sit', 'satisfied', 'scarcely', 'seem', 'seem', 'set', 'shake', 'show', 'silent_silently', 'situated', 'slice', 'small', 'small', 'small', 'smile_smile', 'sneeze', 'soil', 'soon', 'space', 'speedily_spend', 'spread', 'stare', 'stark', 'still', 'stand', 'store', 'stove', 'stow', 'strange', 'stream_stream', 'stumble', 'sufficient', 'suppose', 'taken_taking', 'talk', 'tells_tempere', 'tent_tent', 'tent_tent', 'thereafter', 'think', 'think', 'thoroughly', 'though', 'thus', 'time', 'time', 'time', 'tiny', 'tired', 'tell', 'tell', 'take', 'totally', 'tour', 'track', 'turn', 'turn', 'turn', 'typical', 'uncommon', 'understand', 'unfriendly', 'unnamed', 'unpeopled', 'vacation', 'venture', 'village_village', 'village_village', 'village', 'walked_walking', 'want', 'waste', 'waveshaped', 'way', 'way', 'weather', 'go', 'go', 'west', 'whole', 'whole', 'whole', 'wilderness', 'wish', 'work', 'world', 'year', 'yet'], ['afterwards', 'good', 'hitherto', 'irish', 'kraighten', 'let', 'let', 'perhaps', 'presently', 'return', 'see', 'seem', 'somehow', 'suddenly', 's', 'go', 'abrupt', 'absence', 'againanyhow', 'agree', 'aimlessly', 'air_air', 'along', 'also', 'also', 'always', 'amazement', 'ancient', 'answer', 'appetite', 'arrive', 'artistmade', 'aside', 'asleep', 'assemble', 'away', 'awhile', 'bank', 'begin', 'belief', 'belonging', 'bent', 'big', 'bit_bit', 'blank', 'blessing', 'bring', 'bushes_bushe', 'bushes_bushe', 'bush', 'come', 'come', 'come', 'come', 'certain_certain', 'certainly', 'chat', 'chest', 'column', 'come', 'come', 'come', 'comfortably', 'companion', 'companionwho', 'comprehension', 'confidently', 'conscious', 'conviction', 'creel', 'curiously', 'dark', 'day', 'day', 'day', 'days_day', 'depart', 'direction', 'disagreeably', 'dismal', 'doing', 'dress', 'drone', 'early', 'ear', 'earth', 'eat', 'endvanishe', 'enough', 'enough', 'enter', 'evening', 'ever', 'evidence', 'experience', 'explain', 'expression', 'eye', 'eye', 'face', 'face', 'fact', 'far', 'feature', 'feed', 'feeling', 'feel', 'find', 'fine', 'firstrate', 'fish_fish', 'fished_fishe', 'fix', 'flat', 'follow', 'forward_forward', 'forward_forward', 'friend_friend', 'friendly', 'fruittree', 'garden_garden', 'gazed_gaze', 'glance', 'gloomy', 'go', 'go', 'go', 'go', 'good', 'get', 'grateful', 'great', 'great', 'great', 'great', 'grow', 'ground', 'group', 'grow', 'halt', 'hand', 'happily', 'head', 'head', 'hide', 'high', 'high', 'hold', 'hour', 'however', 'imagine', 'inaction', 'innumerable', 'interestedly', 'investigate', 'justice', 'know', 'lay', 'leisurely', 'light', 'line', 'listen', 'little', 'lock', 'loneliness', 'long', 'long', 'long', 'look', 'look', 'look', 'look', 'lowland', 'lunch_lurke', 'make', 'make', 'make', 'make', 'make', 'many', 'matted', 'matter', 'matter', 'meddle', 'midday', 'mile', 'minute', 'mist', 'moment', 'moment', 'morning', 'mountain', 'move', 'much', 'mutter', 'nervousness', 'nod', 'noise', 'note', 'oasis', 'occasion', 'old', 'onwards', 'onwards', 'opposite', 'overhaul', 'perhaps', 'perhaps', 'piece', 'place', 'place', 'place', 'place', 'place', 'plainer_plainly', 'pleased', 'pretty', 'prosecute', 'push', 'quiet_quietly', 'rainbow', 'ramble', 'rather', 'remainder', 'reply', 'respectful', 'resume', 'rightaway', 'riot', 'river', 'roar', 'rock_rock', 'rod', 'rose_rose', 'rough', 'rouse', 'say', 'say', 'say', 'say', 'say', 'scenery', 'search', 'secure', 'see', 'see', 'see', 'seem', 'seem', 'seem', 'seem', 'seem', 'see', 'select', 'sense', 'set', 'settle', 'several', 'several', 'shaded', 'shivery', 'shone', 'sign', 'silent_silent', 'sketch', 'slope', 'smoke', 'sombre', 'somewhat', 'somewhere', 'soon', 'sound', 'spend', 'spray', 'stared_stare', 'start', 'steadily', 'stone', 'stop', 'suggest', 'sun', 'tackle', 'take', 'tangled', 'tent_tent', 'thing', 'think', 'though', 'though', 'though', 'thought', 'think', 'thoughtfully', 'time', 'time', 'tired', 'top', 'trace', 'trees_tree', 'trees_tree', 'tree', 'trudge', 'turn', 'uncanny', 'upon_upon', 'upon_upon', 'upon_upon', 'upon_upstream', 'venture', 'vigorously', 'villager', 'visit', 'walk', 'wander', 'warm', 'warning', 'watch_watched', 'way', 'way', 'well', 'go', 'go', 'go', 'go', 'whatsoever', 'wilderness', 'within', 'work', 'worth'], ['arrive', 'come', 'curiously', 'good', 'look', 'presently', 'reach', 'steadily', 'take', 's', 'well', 'able', 'abyss_abys', 'abyss_abyss', 'abyss', 'actually', 'almost', 'almost', 'almost', 'almost', 'along', 'arm', 'attribute', 'away', 'awed', 'back', 'back', 'back', 'bank', 'bellow', 'belong', 'bewilderment', 'boil', 'bone', 'book', 'break', 'brushing', 'building', 'bury', 'burst', 'call', 'come', 'come', 'come', 'come', 'come', 'come', 'castle', 'cataract', 'circle', 'closely', 'come', 'come', 'commence', 'complete', 'continue', 'continuous', 'crumpled_cry', 'damage', 'damp', 'debris', 'deeps_delay', 'depths_depth', 'desk', 'destroy', 'discover', 'distant', 'dizzy', 'double', 'dry', 'due', 'earth', 'end', 'enough', 'erect', 'ever', 'ever', 'ever', 'evidently', 'examine_examine', 'exploration', 'extreme', 'eye', 'fact', 'fairly', 'fall_fallen', 'fall', 'feel', 'feet_feet', 'feetand_felt', 'field', 'fill', 'finger_finger', 'first', 'first', 'first', 'flash', 'form', 'forward', 'find', 'find', 'find', 'fragment', 'friend', 'front_frothe', 'give', 'give', 'gaze', 'glance', 'go', 'grand', 'great', 'great', 'great', 'grew_grew', 'ground', 'hand', 'hand', 'hard', 'heading', 'heap', 'hear', 'hour', 'however', 'huge', 'hurry', 'hurt', 'impression', 'intolerable', 'jagged', 'jut', 'jut', 'land', 'leave', 'leave', 'legible', 'light', 'literally', 'look', 'look', 'look', 'look', 'look', 'louder', 'make', 'make', 'make', 'many', 'mass', 'mean', 'midair', 'minute_minutely', 'mistaken', 'mistlike', 'monster', 'mouth_mouth', 'much', 'muddied', 'near_neare', 'near', 'nearly_neat', 'new', 'next', 'noise', 'note', 'object', 'observe', 'oldfashioned', 'open', 'opened_opene', 'opposite', 'outer_out', 'outer', 'pace', 'pages_page', 'part', 'part', 'part', 'perch_perche', 'perfect', 'perhaps', 'place', 'point', 'prize', 'probably', 'proceed', 'prodigious', 'project', 'promontory', 'protect', 'put', 'put', 'put', 'put', 'queer', 'quite', 'quite', 'rather', 'reach', 'rest', 'rise_rise', 'roaring_rock', 'rock_rock', 'rock', 'rocky', 'round_round', 'round_round', 'rubble_ruin', 'ruin', 'ruincrowne', 'ruined_ruin', 'run', 'safely_safety', 'say', 'see', 'see', 'see', 'scrutinise', 'search', 'second_securely', 'see', 'see', 'seem', 'shoulder', 'show', 'shroud', 'shrub', 'side', 'side', 'side', 'side', 'side', 'sight_sight', 'sign', 'six_sixty', 'small', 'soon', 'space', 'spoil', 'spot', 'spout', 'stand', 'stare', 'start_starte', 'still', 'stones_stone', 'stone', 'stand', 'straight', 'structure', 'substantially', 'sudden_suddenly', 'suppose', 'surface', 'symmetry', 'task', 'tell', 'terror', 'thence', 'thick', 'thing', 'thing', 'thing', 'though', 'though', 'though', 'think', 'thunder', 'together', 'top', 'touch', 'tour', 'towards_toward', 'tower', 'trees_tree', 'tumedto', 'turn', 'undoubtedly', 'unexpectedly', 'unknown', 'upheaped', 'usinto', 'view', 'volume', 'walk', 'go', 'go', 'go', 'whole', 'within', 'within', 'witness', 'wonder', 'wood', 'work', 'writing', 'yawn', 'yet', 'yet', 'yet', 're'], ['chapter', 'creation', 'let', 'look', 'onwards', 'plain', 'pepper', 'perhaps', 'presently', 'silence', 'suddenly', 'supper', 's', 'tonnison', 'yet', 'yet', 've', 'able', 'able', 'access', 'advice', 'air', 'alone_alone', 'alone', 'also', 'also', 'ancient', 'answer_answered', 'anxiety_anxious', 'appear', 'around', 'ask', 'ask', 'ask', 'away', 'away', 'away', 'away', 'back', 'back', 'bad', 'bare', 'begin', 'begin', 'begin', 'bent', 'well', 'well', 'buy', 'bowel', 'branch', 'break', 'bubble', 'building', 'bushes_bushe', 'bush', 'come', 'come', 'come', 'come', 'come', 'cataract', 'catch', 'caution', 'century', 'chasm', 'clear_cleare', 'close', 'clumsily', 'come', 'come', 'confine', 'consequently', 'contain', 'continuous', 'countryside', 'couple', 'diary', 'direction', 'distant_distant', 'dog', 'dread', 'earth', 'eat', 'enable', 'enough', 'even', 'evening', 'express', 'fancy', 'foot', 'figure', 'float', 'follow', 'forward', 'friend', 'full', 'furtively', 'garden', 'get', 'gigantic', 'glanced_glanced', 'go', 'go', 'get', 'great', 'ground', 'gurgle', 'half', 'hand', 'hand', 'hate', 'haunt', 'hear', 'hear', 'heel', 'hide', 'holding', 'hold', 'horrible', 'hour', 'housekeeper', 'huge', 'hundred', 'hurry', 'inhabit', 'involuntarily', 'keep', 'keep', 'kick', 'kind', 'know', 'know', 'know', 'lake', 'last', 'later', 'learn', 'least', 'least', 'leave', 'leave', 'leave', 'listen', 'light', 'little', 'live_live', 'live', 'loneliness', 'long', 'long', 'look', 'look', 'loud', 'lunch', 'mad', 'make', 'make', 'man', 'manuscript', 'many', 'meal', 'midday', 'mighty', 'mind', 'minute_minute', 'moment', 'mood', 'mouth', 'move_move', 'move', 'much', 'much', 'needless', 'nervously', 'night', 'nod', 'noise_noise', 'north', 'notice', 'occasional', 'oif', 'old', 'old', 'old', 'old', 'old', 'opened_opene', 'page', 'peasantry', 'pipe', 'pit_pit', 'place', 'place', 'place', 'place', 'placei', 'please', 'prepare', 'propensity', 'put', 'put', 'question', 'quietness', 'raw', 'reach', 'reach', 'read_read', 'realise', 'record_record', 'reputation', 'rest', 'ridiculously', 'rise', 'rockiness', 'rustle', 'safely', 'say', 'say', 'say', 'say', 'say', 'say', 'say', 'see', 'say', 'say', 'seat', 'seem', 'seem', 'seem', 'see', 'see', 'sense_sense', 'servantsi', 'set', 'shake', 'sharply', 'sheer', 'silent', 'sister', 'skip', 'slowly', 'snap', 'space', 'speak_speak', 'spend', 'spoke_spoke', 'spout', 'stare', 'start', 'steadily', 'step', 'still', 'stir', 'stand', 'strained', 'strange', 'sudden_suddenly', 'suggest', 'sun', 'superstitious', 'tale', 'talk', 'tangle', 'tent_tent', 'tent', 'thing', 'thing', 'thing', 'thingsyou', 'thin', 'think', 'thought', 'time', 'time', 'title', 'together', 'tell', 'tell', 'top', 'towards_toward', 'trees_tree', 'trees_tree', 'tree', 'trunk', 'turn', 'twice', 'twig', 'understand', 'unkempt', 'vile', 'voice', 'wailing_waile', 'watersilent', 'way', 'way', 'wealth', 'well', 'go', 'wilderness', 'windyet', 'woods_wood', 'work', 'world', 'yard', 'year', 'year', 'yet']]\n"
     ]
    }
   ],
   "source": [
    "#Run functions to remove stopwords, make bigrams, and lemmatize text\n",
    "data_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_nostops)\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dictionary and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Ideal # of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/gensim/__init__.py\", line 11, in <module>\n",
      "    from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/gensim/parsing/__init__.py\", line 4, in <module>\n",
      "    from .preprocessing import (  # noqa:F401\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/gensim/parsing/preprocessing.py\", line 26, in <module>\n",
      "    from gensim import utils\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/gensim/utils.py\", line 36, in <module>\n",
      "    from smart_open import open\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/smart_open/__init__.py\", line 34, in <module>\n",
      "    from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/smart_open/smart_open_lib.py\", line 35, in <module>\n",
      "    from smart_open import doctools\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/smart_open/doctools.py\", line 21, in <module>\n",
      "    from . import transport\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/smart_open/transport.py\", line 100, in <module>\n",
      "    register_transport('smart_open.http')\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/smart_open/transport.py\", line 49, in register_transport\n",
      "    submodule = importlib.import_module(submodule)\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/smart_open/http.py\", line 16, in <module>\n",
      "    import requests\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py\", line 44, in <module>\n",
      "    import chardet\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/chardet/__init__.py\", line 20, in <module>\n",
      "    from .universaldetector import UniversalDetector\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/chardet/universaldetector.py\", line 47, in <module>\n",
      "    from .mbcsgroupprober import MBCSGroupProber\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/chardet/mbcsgroupprober.py\", line 32, in <module>\n",
      "    from .sjisprober import SJISProber\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/chardet/sjisprober.py\", line 30, in <module>\n",
      "    from .chardistribution import SJISDistributionAnalysis\n",
      "  File \"/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/chardet/chardistribution.py\", line 36, in <module>\n",
      "    from .jisfreq import (JIS_CHAR_TO_FREQ_ORDER, JIS_TABLE_SIZE,\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 874, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 973, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Can take a long time to run.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_list, coherence_values \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_coherence_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mcompute_coherence_values\u001b[0;34m(dictionary, corpus, texts, limit, start, step)\u001b[0m\n\u001b[1;32m     21\u001b[0m     model_list\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     22\u001b[0m     coherencemodel \u001b[38;5;241m=\u001b[39m CoherenceModel(model\u001b[38;5;241m=\u001b[39mmodel, texts\u001b[38;5;241m=\u001b[39mtexts, dictionary\u001b[38;5;241m=\u001b[39mdictionary, coherence\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_v\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     coherence_values\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcoherencemodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_list, coherence_values\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/coherencemodel.py:615\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coherence\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;124;03m\"\"\"Get coherence value based on pipeline parameters.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m \n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m     confirmed_measures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coherence_per_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_measures(confirmed_measures)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/coherencemodel.py:575\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    573\u001b[0m     segmented_topics \u001b[38;5;241m=\u001b[39m measure\u001b[38;5;241m.\u001b[39mseg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopics)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmented_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(with_std\u001b[38;5;241m=\u001b[39mwith_std, with_support\u001b[38;5;241m=\u001b[39mwith_support)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;129;01min\u001b[39;00m BOOLEAN_DOCUMENT_BASED \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_w2v\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/coherencemodel.py:547\u001b[0m, in \u001b[0;36mCoherenceModel.estimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_w2v\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    545\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeyed_vectors\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/topic_coherence/probability_estimation.py:156\u001b[0m, in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    154\u001b[0m     accumulator \u001b[38;5;241m=\u001b[39m ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n\u001b[1;32m    155\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to estimate probabilities from sliding windows\u001b[39m\u001b[38;5;124m\"\u001b[39m, accumulator)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:443\u001b[0m, in \u001b[0;36mParallelWordOccurrenceAccumulator.accumulate\u001b[0;34m(self, texts, window_size)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccumulate\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts, window_size):\n\u001b[0;32m--> 443\u001b[0m     workers, input_q, output_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue_all_texts(input_q, texts, window_size)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:477\u001b[0m, in \u001b[0;36mParallelWordOccurrenceAccumulator.start_workers\u001b[0;34m(self, window_size)\u001b[0m\n\u001b[1;32m    475\u001b[0m     accumulator \u001b[38;5;241m=\u001b[39m PatchedWordOccurrenceAccumulator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevant_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary)\n\u001b[1;32m    476\u001b[0m     worker \u001b[38;5;241m=\u001b[39m AccumulatingWorker(input_q, output_q, accumulator, window_size)\n\u001b[0;32m--> 477\u001b[0m     \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     workers\u001b[38;5;241m.\u001b[39mappend(worker)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m workers, input_q, output_q\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Topic Model with 20 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20,\n",
    "                                           random_state=100,\n",
    "                                           update_every=2,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (1,\n",
      "  '0.015*\"higgins_higgins\" + 0.014*\"station_station\" + 0.010*\"scrambled\" + '\n",
      "  '0.009*\"plane_plane\" + 0.006*\"driver_driver\" + 0.006*\"mobile\" + '\n",
      "  '0.006*\"paul_pauls\" + 0.005*\"gang\" + 0.005*\"jeep_jeep\" + 0.004*\"gut\"'),\n",
      " (2,\n",
      "  '0.005*\"niche\" + 0.004*\"returned_revealed\" + 0.002*\"diabolical\" + '\n",
      "  '0.002*\"genial\" + 0.002*\"beckoning\" + 0.001*\"speed_speeding\" + '\n",
      "  '0.001*\"serenely\" + 0.000*\"slipping_slope\" + 0.000*\"kali_kali\" + '\n",
      "  '0.000*\"dam_dam\"'),\n",
      " (3,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (4,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (5,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (6,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (7,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (8,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (9,\n",
      "  '0.016*\"could\" + 0.015*\"would\" + 0.012*\"eyes\" + 0.012*\"thought\" + '\n",
      "  '0.011*\"like\" + 0.009*\"knew\" + 0.009*\"still\" + 0.008*\"face\" + 0.008*\"us\" + '\n",
      "  '0.008*\"never\"'),\n",
      " (10,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (11,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (12,\n",
      "  '0.022*\"would\" + 0.015*\"one\" + 0.014*\"could\" + 0.011*\"time\" + 0.008*\"even\" + '\n",
      "  '0.006*\"first\" + 0.006*\"new\" + 0.006*\"might\" + 0.006*\"two\" + 0.005*\"years\"'),\n",
      " (13,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (14,\n",
      "  '0.059*\"said\" + 0.017*\"dont\" + 0.016*\"know\" + 0.015*\"get\" + 0.014*\"im\" + '\n",
      "  '0.012*\"got\" + 0.012*\"like\" + 0.011*\"well\" + 0.011*\"right\" + 0.011*\"think\"'),\n",
      " (15,\n",
      "  '0.008*\"slope_slope\" + 0.007*\"pistol_pistol\" + 0.004*\"ripple\" + '\n",
      "  '0.004*\"cords\" + 0.004*\"bum_bum\" + 0.003*\"foolishness\" + '\n",
      "  '0.003*\"breathed_breathing\" + 0.003*\"chop\" + 0.002*\"clammy\" + '\n",
      "  '0.002*\"arrayed\"'),\n",
      " (16,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (17,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"'),\n",
      " (18,\n",
      "  '0.013*\"back\" + 0.008*\"like\" + 0.008*\"around\" + 0.008*\"one\" + 0.007*\"light\" '\n",
      "  '+ 0.007*\"two\" + 0.007*\"away\" + 0.007*\"came\" + 0.006*\"behind\" + '\n",
      "  '0.006*\"turned\"'),\n",
      " (19,\n",
      "  '0.000*\"faith_fame\" + 0.000*\"experienced_explore\" + '\n",
      "  '0.000*\"increases_inevitable\" + 0.000*\"galactics_game\" + 0.000*\"flypaper\" + '\n",
      "  '0.000*\"modernand\" + 0.000*\"climaxalien\" + 0.000*\"continuing_contrast\" + '\n",
      "  '0.000*\"alarm_alcove\" + 0.000*\"envied_essential\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -14.519557187409122\n",
      "\n",
      "Coherence Score:  0.4020247749987556\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megankane/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "\n",
    "#vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_display.py:222\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(data, local, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay: specified urls are ignored when local=True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md3_url\u001b[39m\u001b[38;5;124m'\u001b[39m], kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mldavis_url\u001b[39m\u001b[38;5;124m'\u001b[39m], kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mldavis_css_url\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m write_ipynb_local_js()\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HTML(\u001b[43mprepared_data_to_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_display.py:177\u001b[0m, in \u001b[0;36mprepared_data_to_html\u001b[0;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, visid):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisid must not contain spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m template\u001b[38;5;241m.\u001b[39mrender(visid\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(visid),\n\u001b[1;32m    174\u001b[0m                        visid_raw\u001b[38;5;241m=\u001b[39mvisid,\n\u001b[1;32m    175\u001b[0m                        d3_url\u001b[38;5;241m=\u001b[39md3_url,\n\u001b[1;32m    176\u001b[0m                        ldavis_url\u001b[38;5;241m=\u001b[39mldavis_url,\n\u001b[0;32m--> 177\u001b[0m                        vis_json\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    178\u001b[0m                        ldavis_css_url\u001b[38;5;241m=\u001b[39mldavis_css_url)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py:471\u001b[0m, in \u001b[0;36mPreparedData.to_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_json\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNumPyEncoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/json/__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/utils.py:150\u001b[0m, in \u001b[0;36mNumPyEncoder.default\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mfloat64) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(obj)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJSONEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ]
    }
   ],
   "source": [
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare: LDA Mallet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "!unzip mallet-2.0.8.zip\n",
    "mallet_path = '/content/mallet-2.0.8/bin/mallet' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get ideal number of topics\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get mallet path and run new topic model\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create visualization\n",
    "converted_ = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
    "vis_mallet = gensimvis.prepare(converted_, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display visualization \n",
    "pyLDAvis.display(vis_mallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Topics Per Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
