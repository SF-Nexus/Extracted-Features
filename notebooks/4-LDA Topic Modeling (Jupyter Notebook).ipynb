{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This code replicates the Google Colab topic modeling notebook for use on Jupyter Notebook or another local Python environment.\n",
    "\n",
    "Below, Gensim is used to create LDA topic models of a pre-loaded dataframe of texts. Methods for evaluating topic coherence and analyzing topic output are also demonstrated. \n",
    "\n",
    "This code is adapted from [Intro to Topic Modeling with Gensim and pyLDAvis](https://github.com/hawc2/text-analysis-with-python/blob/master/Topic_Modeling.ipynb) and works well with input from from the Text Sectioning and Disaggregation code from [this repository](https://github.com/SF-Nexus/Extracted-Features).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages\n",
    "This code requires three main packages:\n",
    "- **NLTK:** Cleaning disaggregated data\n",
    "- **Gensim:** Preprocessing data and creating word embeddings, coherence models and topic models\n",
    "- **LDAvis:** Visualizing topic models\n",
    "\n",
    "Several other packages for wrangling and processing the data, such as io and pandas, will also be installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve and Convert Corpus to Data Frame\n",
    "\n",
    "This code requires the upload of a previously created dataframe which contains a corpus of disaggregated texts. For optimal use, this dataframe should also contain labels associated with each text (e.g. book or chapter numbers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get current working directory \n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#Change working directory\n",
    "path = os.chdir(\"/home/dssadmin/Desktop/ExtractedFeatures_test_directory\")\n",
    "\n",
    "#Upload dataframeâˆš\n",
    "df = pd.read_csv('chapter_chunks_bow_output.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any empty cells\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add values in Text column to new list (for further cleaning)\n",
    "data = df.Text.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Texts\n",
    "Below are several preprocessing measures to help improve quality of text used for the topic model. Some may or may not be useful depending on specifications of corpus or goals of topic modeling:\n",
    "- **Stopword Removal:** Removes globally common words from corpus which may skew topics; some stopwords may be useful for analysis and some studies show that removing more than the top 10 most common words does not significantly impact topic model results \n",
    "- **Bigrams/Trigram Modeling:** Recognizes common 2- and 3-word phrases to prevent disambiguation (e.g. spaceship vs. space, ship); may misidentify phrases especially in corpora with large/uncommon vocabularies like sci-fi\n",
    "- **Lemmatization:** Transforms words to root form to aid recognition; some studies show may be redundant, unnecessary or even inaccurate in conflating differing words with same root meanings \n",
    "\n",
    "*Read More:*\n",
    "\n",
    "https://maria-antoniak.github.io/2022/07/27/topic-modeling-for-the-people.html\n",
    "\n",
    "https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Add further stopwords by simply \"appending\" desired words to dictionary\n",
    "stop_words.append('CHAPTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to perform simple preprocessing on text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run processing function on texts\n",
    "data_words = list(sent_to_words(data))\n",
    "#print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define stopword removal\n",
    "def remove_stopwords(texts):\n",
    "   return [[word for word in simple_preprocess(str(doc))\n",
    "if word not in stop_words] for doc in texts]\n",
    "\n",
    "#Define function to make bigrams\n",
    "def make_bigrams(texts):\n",
    "   return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "#def make_trigrams(texts):\n",
    "#   return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "#Define function to lemmatize texts\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "   texts_out = []\n",
    "   for sent in texts:\n",
    "     doc = nlp(\" \".join(sent))\n",
    "     texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "   return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Define bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=1, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run functions to remove stopwords, make bigrams, and lemmatize text\n",
    "data_words = remove_stopwords(data_words)\n",
    "#data_words_bigrams = make_bigrams(data_nostops)\n",
    "#data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "#print(data_lemmatized[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dictionary and Corpus\n",
    "Once the dataset is cleaned, two inputs must be created to run the topic model. Creating the dictionary maps every word in the corpus with a unique id number. The variable corpus is calculated by determining the frequency of each word in the document. Another optional cleaning measure can be used here--removing words that are extremely rare (existing in < n number of texts) or common (existing in > n number of texts).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dictionary\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(data_words)\n",
    "\n",
    "#Calculate term document frequency for each word in dataset\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data_words]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL cleaning: filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "#dictionary.filter_extremes(no_below=2, no_above=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of unique tokens and documents\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Optimal Number of Topics\n",
    "\n",
    "Calculating topic model coherence (i.e. similarity between the highest-scoring words in each topic) is one way to assess the optimal number of topics to use when creating and visualizing topic models. Below are two methods of calculating coherence: **C_V coherence**, which is calculated based on word co-occurences, and **U_Mass coherence**, which is calculated based on how frequently documents containing high-scoring words co-occur in the corpus. Both have been used in prior research and either may yield better results, depending on corpus specifications.\n",
    "\n",
    "Additional readings: \n",
    "\n",
    "https://aclanthology.org/D12-1087.pdf \n",
    "\n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C_V Coherence Calculations\n",
    "\n",
    "The function below calculates C_V coherence scores for n topic models run based on set parameters. Coherence is calculated on a range of 0 < x < 1 where higher-scoring models are assumed more coherent. For example, a model with a score of .50 is more coherent than a model with a .25 score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define list for model and coherence values\n",
    "coherence = []\n",
    "\n",
    "#Find coherence for set range of models and append to list\n",
    "for k in range(2,200):\n",
    "    print('Round: '+str(k))\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    ldamodel = Lda(corpus, num_topics=k, \\\n",
    "               id2word = dictionary, eval_every = None)\n",
    "    \n",
    "    cm = gensim.models.coherencemodel.CoherenceModel(\\\n",
    "         model=ldamodel, texts=data_words,\\\n",
    "         dictionary=dictionary, coherence='c_v')   \n",
    "                                                \n",
    "    coherence.append((k,cm.get_coherence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose coherence data\n",
    "x, y = np.array(coherence).T\n",
    "  \n",
    "      \n",
    "# plot our list in X,Y coordinates\n",
    "optimal, = plt.plot(x, y)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get coherence score for each num topics sorted from highest to lowest\n",
    "#Highest value will be optimal number of topics\n",
    "Data = {'Num Topics': optimal.get_xdata(), 'Coherence': optimal.get_ydata()}\n",
    "type(Data)\n",
    "df_optimal = pd.DataFrame.from_dict(Data)\n",
    "df_optimal.sort_values(by='Coherence',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U_Mass Coherence Calculation\n",
    "The function below calculates U_Mass coherence scores of n topic models run based on set parameters. Coherence is calculated on a range of -14 < x < 14 where lower-scoring models are more coherent. For example, a model with a score of -.4 is less coherent than a model with a -.9 score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define list for model and coherence values\n",
    "coherence2 = []\n",
    "\n",
    "#Find coherence for set range of models and append to list\n",
    "for k in range(2,100):\n",
    "    print('Round: '+str(k))\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    ldamodel = Lda(corpus, num_topics=k, \\\n",
    "               id2word = dictionary, eval_every = None)\n",
    "    \n",
    "    cm = gensim.models.coherencemodel.CoherenceModel(\\\n",
    "         model=ldamodel, texts=data_words,\\\n",
    "         dictionary=dictionary, coherence='u_mass')   \n",
    "                                                \n",
    "    coherence2.append((k,cm.get_coherence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose coherence data\n",
    "x, y = np.array(coherence2).T\n",
    "        \n",
    "# plot our list in X,Y coordinates\n",
    "optimal2, = plt.plot(x, y)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get coherence score for each num topics sorted from highest to lowest\n",
    "#Lowest value will be optimal number of topics\n",
    "Data = {'Num Topics': optimal2.get_xdata(), 'Coherence': optimal2.get_ydata()}\n",
    "type(Data)\n",
    "df_optimal2 = pd.DataFrame.from_dict(Data)\n",
    "df_optimal2.sort_values(by='Coherence',ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Topic Models with Optimal Parameters\n",
    "Input the parameters of the topic model and run. The model has multiple parameters, including: \n",
    "- num_topics: Number of topics the model will generate (default = 100)\n",
    "- chunksize: Number of documents processed at a time (default = 2000)\n",
    "- passes: Number of times model is trained on corpus (default = 1)\n",
    "- iterations: Number of times model \"loops\" over each document (default = 50)\n",
    "\n",
    "Calculating coherence (above) helps determine topic number, and other methods can be used to determine appropriate values for the other parameters. \n",
    "\n",
    "**Chunk size:** Setting chunk size to a larger number than that of documents in the model ensures that all documents are processed at once (though this requires enough memory space). \n",
    "\n",
    "**Passes and Iterations:** A common way to determine the best number of passes and iterations is by training a topic model and checking the \"log\" to see the document convergence rate (what percentage of topic/word assignments attain stability). If convergence is low, increase number of passes and interations. \n",
    "\n",
    "In general, as chunksize increases, passes and iterations should increase as well. Also keep in mind that corpus size may effect number of topics--in the cases of smaller corpora, using too many topics will likely make them too general OR too limited to the context of only one text. Consider running multiple models and comparing coherence, perplexity, and top words per topic. \n",
    "\n",
    "Additional reading: \n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#12buildingthetopicmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import logging to gauge passes and iterations; will output file to working directory\n",
    "import logging\n",
    "logging.basicConfig(filename='gensim.log',\n",
    "                    format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 100\n",
    "chunksize = 1000\n",
    "passes = 20\n",
    "iterations = 40\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=1\n",
    ")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute C_V Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test LDA model multicore\n",
    "from gensim.models import ldamulticore\n",
    "from gensim import corpora, models\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 100\n",
    "chunksize = 1000\n",
    "passes = 20\n",
    "iterations = 40\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "lda_model2 = models.LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    workers = cores-1,\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=1\n",
    ")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model2.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute C_V Coherence Score\n",
    "coherence_model_lda2 = CoherenceModel(model=lda_model2, texts=data_words, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Topic Model Output\n",
    "Once the model has been run, it is possible to retrieve the top words in each topic and visualing the model. These are methods can help further assess model coherence and point to future directions for analysis:\n",
    "- **Top Words Per Topic:** Evaluate to what extent each topic contains semantically similar words, how/why these words might be meaningful in context of corpus\n",
    "- **Visualizations:** Determine topic relatedness (how far apart topic circles are on plane) and topic prevalence (how large circles are corresponds to topic prevaence in corpus)\n",
    "\n",
    "Additional reading: \n",
    "\n",
    "https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#15visualizethetopicskeywords\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print n number of words in each topic\n",
    "for idx, topic in lda_model.print_topics(num_words=20):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary with topics and words\n",
    "my_dict = {\"Topic\":[],\"Words\":[]}\n",
    "for idx, topic in lda_model.print_topics(num_words=20):\n",
    "    my_dict[\"Topic\"].append(idx)\n",
    "    my_dict[\"Words\"].append(topic)\n",
    "\n",
    "#Convert dictionary to dataframe\n",
    "topics_df = pd.DataFrame.from_dict(my_dict)\n",
    "topics_df.head()\n",
    "\n",
    "#Download dataframe as csv\n",
    "topics_df.to_csv('topic_word_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define word cloud function\n",
    "from wordcloud import WordCloud \n",
    "def create_wordcloud(model, topic):\n",
    "    text = {word: value for word, value in model.show_topic(topic)}\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000)\n",
    "    wc.generate_from_frequencies(text)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic\" + \" \"+ str(topic))\n",
    "    plt.show()\n",
    "    \n",
    "#Ignore depreciation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Create word clouds\n",
    "for i in range(1,num_topics):\n",
    "    create_wordcloud(lda_model, topic=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create visualization of topic model above \n",
    "%matplotlib inline\n",
    "import pyLDAvis.gensim_models\n",
    "vis = pyLDAvis.gensim_models.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary, mds='mmds')\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.show(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'vis200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Top Topics Per Document\n",
    "\n",
    "Find the topic with highest percentage in each document in corpus. \n",
    "\n",
    "Additional reading: \n",
    "https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function that retrieves dominant topic for each document and puts in dataframe\n",
    "def format_topics_texts(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    text_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                text_topics_df = text_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    text_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    text_topics_df = pd.concat([text_topics_df, contents], axis=1)\n",
    "    return(text_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run dominant topic function on corpus\n",
    "df_topic_texts_keywords = format_topics_texts(ldamodel=lda_model, corpus=corpus, texts=data_words)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_texts_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Top Documents Per Topic\n",
    "Calculate the top documents attributed to each topic in the model. \n",
    "\n",
    "Additional reading:\n",
    "\n",
    "https://stackoverflow.com/questions/63777101/topic-wise-document-distribution-in-gensim-lda\n",
    "\n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb \n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/#7.-The-most-representative-sentence-for-each-topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Book + Chapter labels to dataframe for easier ID\n",
    "doc_names = df['Book + Chapter']\n",
    "df_topic_texts_keywords = df_topic_texts_keywords.join(doc_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most representative text for each topic \n",
    "#Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "#Create new dataframe and group topic keywords by dominant topic column\n",
    "topics_sorted_df = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_texts_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "#Sort data by percent contribution and select highest n values for each topic\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    topics_sorted_df = pd.concat([topics_sorted_df, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Index of new df\n",
    "topics_sorted_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "topics_sorted_df.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\", \"Text Name\"]\n",
    "topics_sorted_df = topics_sorted_df.reindex(columns=['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text Name\", \"Representative Text\"])\n",
    "# Show\n",
    "topics_sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download top doc per topic to dataframe\n",
    "topics_sorted_df.to_csv('top_doc_per_topic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Word2Vec (Work In Progress)\n",
    "\n",
    "https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92 \n",
    "\n",
    "https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/\n",
    "\n",
    "https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "#!pip install spacy\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load corpus (can't use disaggregated data here!)\n",
    "\n",
    "##Get current working directory \n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#Change working directory\n",
    "path = os.chdir(\"/home/dssadmin/Desktop/ExtractedFeatures_test_directory\")\n",
    "\n",
    "#Upload dataframeâˆš\n",
    "df = pd.read_csv('chapters_sample.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#essays_grades_master['ID + Score'] = essays_grades_master['ID'].astype(str) + '_' + essays_grades_master['Portfolio Score'].astype(str)\n",
    "#word2vec_test = essays_grades_master[['ID + Score', 'Text_NoHeaders']].copy()\n",
    "word2vec_test = df.copy()\n",
    "word2vec_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get shape of df\n",
    "word2vec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define spaCy function to lemmatize, remove stopwords and non-alphanumeric characters\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove characters\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in word2vec_test['Text'])\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put results in a new dataframe\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use bigrams to detect common phrases\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "#Take list of list of words as input\n",
    "sent = [row.split() for row in df_clean['clean']]\n",
    "\n",
    "#Creates relevant list of phrases from all sentences\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "#Transform the corpus based on the bigrams detected:\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count word frequency\n",
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get most frequent words\n",
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import word2vec\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build word2vecmodel (check how to set parameters in tutorial)\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     #size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build vocab table--digest all words, filter out unique words, do counts on them\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters to train the model\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that the model has been trained, make it more memory efficient\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most similar words to key terms in corpus\n",
    "w2v_model.wv.most_similar(positive=[\"ship\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check similarity between words\n",
    "w2v_model.wv.similarity(\"ship\", 'short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check which word does not belong\n",
    "w2v_model.wv.doesnt_match(['TERM1', 'TERM2', 'TERM3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analogy difference\n",
    "w2v_model.wv.most_similar(positive=[\"TERM1\", \"TERM2\"], negative=[\"TERM3\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "**More Examples of Topic Modeling Research:**\n",
    "\n",
    "*   http://www.digitalhumanities.org/dhq/vol/11/2/000291/000291.html\n",
    "*   http://www.cs.columbia.edu/~blei/papers/Blei2011.pdf\n",
    "*   https://maria-antoniak.github.io/resources/2019_cscw_birth_stories.pdf \n",
    "\n",
    "**More Topic Modeling Tools:**\n",
    "\n",
    "*   https://github.com/polsci/colab-gensim-mallet/blob/master/topic-modeling-with-colab-gensim-mallet.ipynb\n",
    "*   https://github.com/laurejt/authorless-tms \n",
    "*   https://colab.research.google.com/github/kldarek/skok/blob/master/_notebooks/2021-05-27-Topic-Models-Introduction.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
