{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1db2cb",
   "metadata": {},
   "source": [
    "# Running Book NLP on the Sci-Fi Corpus\n",
    "\n",
    "Book NLP is a natural language processing pipeline for large texts. It performs the following tasks: \n",
    " * Part-of-speech tagging\n",
    " * Dependency parsing\n",
    " * Entity recognition\n",
    " * Character name clustering (e.g., \"Tom\", \"Tom Sawyer\", \"Mr. Sawyer\", \"Thomas Sawyer\" -> TOM_SAWYER) and coreference resolution\n",
    " * Quotation speaker identification\n",
    " * Supersense tagging (e.g., \"animal\", \"artifact\", \"body\", \"cognition\", etc.)\n",
    " * Event tagging\n",
    " * Referential gender inference (TOM_SAWYER -> he/him/his)\n",
    "\n",
    "This tutorial runs BookNLP on one and multiple plain text files. BookNLP results (5 different files generated per text) are saved in a folder on your local machine. Some simple analyses (counts, named entity extraction) are also performed. \n",
    "\n",
    "Learn more about BookNLP:\n",
    "* https://github.com/booknlp/booknlp\n",
    "* https://colab.research.google.com/drive/1c9nlqGRbJ-FUP2QJe49h21hB4kUXdU_k?usp=sharing#scrollTo=CVb7N2RDkLVm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843ea55",
   "metadata": {},
   "source": [
    "## Install Packages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c91448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install booknlp\n",
    "from booknlp.booknlp import BookNLP\n",
    "#!pip install spacy\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a4333",
   "metadata": {},
   "source": [
    "## Prep Model and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f80438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters of BookNLP model\n",
    "model_params={\n",
    "    \"pipeline\":\"entity,quote,supersense,event,coref\", \n",
    "    \"model\":\"big\", \n",
    "}\n",
    "\n",
    "booknlp=BookNLP(\"en\", model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44abe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get current working directory \n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#Change working directory\n",
    "path = os.chdir(\"/PATHNAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b192037",
   "metadata": {},
   "source": [
    "## Run Book NLP on One and Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f005d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define file to run BookNLP on\n",
    "inputFile = \"1969_KAMIN_EARTHRIM.txt\"\n",
    "\n",
    "#Set folder where bookNLP results will go\n",
    "outputDir=\"test/\"\n",
    "\n",
    "#Set ID result files\n",
    "idd=\"1969_KAMIN_EARTHRIM\"\n",
    "\n",
    "#Run book NLP on one text\n",
    "booknlp.process(inputFile, outputDir, idd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded15085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define folder where files are stored to run BookNLP on\n",
    "files = [f for f in os.listdir(path) if os.path.isfile(f)]\n",
    "\n",
    "#Loop to run book NLP on each file\n",
    "for f in files: \n",
    "    #Define filename as each file in folder\n",
    "    inputFile = f\n",
    "    #Set folder for booknlp results\n",
    "    outputDir = \"BookNLP_Results/\"\n",
    "    #Set ID as name of each file\n",
    "    idd = f\n",
    "    #Run book nlp\n",
    "    booknlp.process(inputFile, outputDir, idd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf26f64",
   "metadata": {},
   "source": [
    "## Analyze BookNLP Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f07a55",
   "metadata": {},
   "source": [
    "### Convert TSV Files to CSV Files\n",
    "Convert entities, tokens, and supersense files to csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change working directory to folder with BookNLP results\n",
    "path = os.chdir(\"/PATHNAME\")\n",
    "\n",
    "#Define new folder for csv files\n",
    "csvs = \"/PATHNAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a578f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob \n",
    "#Set file endings to look for\n",
    "tokens = 'tokens'\n",
    "supersense = ''\n",
    "endings = ('.entities', 'supersense', 'tokens')\n",
    "\n",
    "#Iterate over original BookNLP files\n",
    "for filename in os.listdir(path):\n",
    "    #Read tsv into a pandas dataframe\n",
    "    if filename.endswith(endings):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\", quoting=3)\n",
    "        #Write the dataframe to a csv file\n",
    "        df.to_csv(os.path.join(csvs, filename + '.csv'), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60221cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zip files in separate folders (for sharing purposes)\n",
    "import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with zipfile.ZipFile('booknlp_entities.zip', 'w') as newzip:\n",
    "    for filepath in glob.glob(\"*.txt.entities.csv\"):\n",
    "        newzip.write(filepath)\n",
    "\n",
    "with zipfile.ZipFile('booknlp_tokens.zip', 'w') as newzip:\n",
    "    for filepath in glob.glob(\"*.txt.tokens.csv\"):\n",
    "        newzip.write(filepath)\n",
    "\n",
    "with zipfile.ZipFile('booknlp_supersense.zip', 'w') as newzip:\n",
    "    for filepath in glob.glob(\"*.txt.supersense.csv\"):\n",
    "        newzip.write(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e8f7b",
   "metadata": {},
   "source": [
    "### Explore BookNLP Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02331833",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.chdir(\"PATHNAME\")\n",
    "\n",
    "#Open NER file for one book\n",
    "for filepath in glob.glob(\"1908_HODGSON_THEHOUSEONTHEBORDERLAND.txt.entities.csv\"):\n",
    "    # Read the .entities.txt file into a pandas dataframe\n",
    "    ner_df = pd.read_csv(filepath)\n",
    "# Print the resulting dataframe\n",
    "ner_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e068c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Open supersense file for one book \n",
    "for filepath in glob.glob(\"1908_HODGSON_THEHOUSEONTHEBORDERLAND.txt.supersense.csv\"):\n",
    "    # Read the .entities.txt file into a pandas dataframe\n",
    "    supersense_df = pd.read_csv(filepath)\n",
    "# Print the resulting dataframe\n",
    "supersense_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1033eb6",
   "metadata": {},
   "source": [
    "### Find All Groups of Named Entities\n",
    "\n",
    "Several groups of named entities may be of interest to researchers: people, organizations, locations, and vehicles.\n",
    "\n",
    "BookNLP tags the following person-based named entities: \n",
    "* PERSON: People, including fictional.\n",
    "\n",
    "BookNLP tags the following person-based named entities: \n",
    "* ORG: Companies, agencies, institutions, etc.\n",
    "\n",
    "BookNLP tags the following place and space-based named entities: \n",
    "* FAC: Buildings, airports, highways, bridges, etc\n",
    "* GPE: Countries, cities, states\n",
    "* LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "\n",
    "BookNLP tags the following vehicle named entities: \n",
    "* VEH: Vehicles\n",
    "\n",
    "This code finds all instances of all groups and appends them to new dataframe column for each text.\n",
    "\n",
    "More about named entity tags: https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fcc815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import to check progress of code\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Get location named entities in each file and append to a dataframe\n",
    "filename_list = []\n",
    "loc_lists = []\n",
    "\n",
    "for filepath in glob.glob(\"*.txt.entities.csv\"):\n",
    "    #Add filename to list of file names\n",
    "    filename_list.append(filepath)\n",
    "    # Read the .entities.txt file into a pandas dataframe\n",
    "    df = pd.read_csv(filepath)\n",
    "    #Define list for locations in df\n",
    "    loc_list = []\n",
    "    #Iterate through dataframe and check if NE category is LOC\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if row['cat'] == 'LOC':\n",
    "            loc_list.append(row['text'])\n",
    "        if row['cat'] == 'GPE':\n",
    "            loc_list.append(row['text'])\n",
    "        if row['cat'] == 'FAC':\n",
    "            loc_list.append(row['text'])\n",
    "    loc_lists.append(loc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca404c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine words into one string\n",
    "import itertools\n",
    "\n",
    "#Combine list of lists into one big list\n",
    "master_loc_list = list(itertools.chain.from_iterable(loc_lists))\n",
    "master_loc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c034b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all words are strings\n",
    "string_loc_list = [str(word) for word in master_loc_list]\n",
    "\n",
    "#Join words in list to string\n",
    "text = ' '.join(string_loc_list)\n",
    "\n",
    "#Remove punctuation\n",
    "text = text.translate(str.maketrans('', ''. string.punctuation))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b9d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stop_words, width=800, height=800, background_color='white', max_words=50)\n",
    "\n",
    "wordcloud.generate(text)\n",
    "\n",
    "#Plot word cloud\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get 20  most frequent locations in corpus\n",
    "from collections import Counter\n",
    "import string \n",
    "\n",
    "#Tokenize text\n",
    "tokens = [token.lower() for token in nltk.word_tokenize(text) if token.lower() not in stop_words]\n",
    "\n",
    "#Count word frequencies\n",
    "word_freq = Counter(tokens)\n",
    "\n",
    "#Get top 20 most common words\n",
    "top_words = dict(word_freq.most_common(20))\n",
    "\n",
    "#Plot word frequencies\n",
    "plt.bar(top_words.keys(), top_words.values())\n",
    "plt.title('Top 20 Most Frequent Words')\n",
    "plt.xlabel('Words')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
