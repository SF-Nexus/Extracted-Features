{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411ecf07",
   "metadata": {},
   "source": [
    "# Cleaning and Basic Text Analysis\n",
    "\n",
    "Methods for cleaning the segmented and disaggregated text files and performing word counts, chapter counts, stopword removal, and other basic frequency calculations and enrichment processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21e349",
   "metadata": {},
   "source": [
    "## Install Packages and Upload Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25792e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "#Get dictionary of English words to keep \n",
    "from nltk.corpus import words\n",
    "#nltk.download('words')\n",
    "#nltk.download('wordnet')\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get current working directory \n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#Change working directory\n",
    "path = os.chdir(\"/PATH\")\n",
    "\n",
    "#Upload dataframeâˆš\n",
    "clean_df = pd.read_csv('chapter_chunks_bow_output.csv')\n",
    "\n",
    "#Drop first column (unnamed)\n",
    "clean_df = clean_df.iloc[: , 1:]\n",
    "\n",
    "#Make text column string values\n",
    "clean_df['Text'] = clean_df['Text'].astype(str)\n",
    "\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369aeac",
   "metadata": {},
   "source": [
    "## Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513575c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase all words\n",
    "clean_df['Clean_Text'] = clean_df['Text'].str.lower()\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(r'[^\\w\\s]+')\n",
    "clean_df['Clean_Text'] = [p.sub('', x) for x in clean_df['Clean_Text'].tolist()]\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove extraneous whitespace using regular expressions\n",
    "clean_df['Clean_Text'] = clean_df['Clean_Text'] .str.replace('  +', ' ', regex=True)\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers and extraneous characters\n",
    "clean_df['Clean_Text'] = clean_df['Clean_Text'] .str.replace('\\d+', '', regex=True)\n",
    "clean_df['Clean_Text'] = clean_df['Clean_Text'] .str.replace('_', '')\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change path to where you want to save the files\n",
    "path = os.chdir(\"/PATH\")\n",
    "\n",
    "#Save cleaned dataframe to working directory\n",
    "clean_df.to_csv('clean_bow_ch_chunks.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63822979",
   "metadata": {},
   "source": [
    "## Advanced Cleaning: Stopword Removal, Lemmatization and Keep Only English Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new dataframe for advanced cleaning\n",
    "adv_clean_df = clean_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "adv_clean_df['Text_NoStops'] = adv_clean_df['Clean_Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "adv_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1efe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define list of words to keep from nltk words\n",
    "#Set function will make processing faster\n",
    "words_list = words.words()\n",
    "my_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words corpus does not contain plural forms, must lemmatize first\n",
    "#nltk.download('omw-1.4')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "#Can choose to lemmatize clean text with or without stopwords\n",
    "adv_clean_df['Text_Lemmas'] = adv_clean_df['Clean_Text'].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in x.split() ]))\n",
    "adv_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda82e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all text to a list of strings\n",
    "adv_clean_df['Text_Lemmas'] = adv_clean_df['Text_Lemmas'].astype(str)\n",
    "data = adv_clean_df.Text_Lemmas.values.tolist()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append each word in list of strings to list of words\n",
    "all_words = []\n",
    "\n",
    "for text in data:\n",
    "    word = text.split()\n",
    "    all_words.append(word)\n",
    "    \n",
    "import itertools\n",
    "all_words_list = list(itertools.chain(*all_words))\n",
    "len(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cb020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only words in lemma list that are also in words corpus\n",
    "adv_clean_df['English_Text'] = adv_clean_df['Text_Lemmas'].apply(lambda x: ' '.join([word for word in x.split() if word in (my_words)]))\n",
    "adv_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all English text to a list of strings\n",
    "adv_clean_df['English_Text'] = adv_clean_df['English_Text'].astype(str)\n",
    "kept_data = adv_clean_df.English_Text.values.tolist()\n",
    "kept_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a284b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append each word in list of strings to list of words\n",
    "kept_words = []\n",
    "\n",
    "for text in kept_data:\n",
    "    word = text.split()\n",
    "    kept_words.append(word)\n",
    "    \n",
    "import itertools\n",
    "kept_words_list = list(itertools.chain(*kept_words))\n",
    "len(kept_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of words that have been removed from the text\n",
    "removed_list = set(all_words_list) - set(kept_words_list)\n",
    "len(removed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9228dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine what words were removed from the text\n",
    "removed_list= list(removed_list)\n",
    "removed_list.sort()\n",
    "removed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf31c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the list of removed words into a dataframe\n",
    "col_name = ['Removed Words']\n",
    "removed_words_df = pd.DataFrame(removed_list, columns = col_name)\n",
    "removed_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change path to where you want to save the files\n",
    "path = os.chdir(\"/PATH\")\n",
    "\n",
    "#Save dataframe with kept words and titles\n",
    "adv_clean_df.to_csv('adv_clean_bow_ch_chunks.csv', index=False)\n",
    "\n",
    "#Saved removed words dataframe to working directory\n",
    "removed_words_df.to_csv('Removed_Words_bow_ch_chunks.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3951d",
   "metadata": {},
   "source": [
    "## Basic Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get new dataframe to work with\n",
    "df_counts = adv_clean_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of words in each chapter chunk\n",
    "#Make sure to use original texts (not cleaned)\n",
    "ch_words = df_counts[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "#Append word counts of each chapter chunk to dataframe\n",
    "df_counts[\"Word Count\"] = ch_words\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f373e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get most frequent words across the dataframe\n",
    "#Use text in English/without stopwords\n",
    "Counter(\" \".join(df_counts[\"Text_NoStops\"]).split()).most_common(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
