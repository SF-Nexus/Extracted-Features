{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Sectioning and Disaggregation in Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMLFVGiOELANBuQaFJUYZa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/Text_Sectioning_and_Disaggregation_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intro to Text Sectioning and Disaggregation\n",
        "\n",
        "This code can be used to clean, section, and disaggregate texts and corpora. Dividing texts into sections (here, chapters or chunks of N lenth) is particularly valuable as a precursor to topic modeling and other forms of computational analysis which perform more accurately when applied to groups of segmented documents from longer texts. \n",
        "\n",
        "The process of disaggregating the words in texts (in this case, by alphabetizing them) also creates data sets that can be shared freely where original texts may not be due to copyright restrictions. \n",
        "\n",
        "This code requires plain txt files as input, either those from this repository's sample_data folder or those from a local machine. It returns csv files with disaggregated text grouped by chapter or chunk of n length."
      ],
      "metadata": {
        "id": "9sOwRC3vIdVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload and Add Text Files To Pandas DataFrame\n",
        "In this section, text files are mounted to Google Drive from the local machine and then loaded into a Pandas DataFrame. Pandas is a fast and relatively easy way to work with large datasets. Though data frames are typically associated with numbers, Pandas also offers many functionalities for [working with textual data. ](https://www.tutorialspoint.com/python_pandas/python_pandas_working_with_text_data.htm) "
      ],
      "metadata": {
        "id": "lFlH8OZYDYS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDahFTGchibT"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet all files to upload\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "c87z_j2mJECQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into dataframe\n",
        "import pandas as pd\n",
        "\n",
        "books = pd.DataFrame.from_dict(uploaded, orient='index')\n",
        "books"
      ],
      "metadata": {
        "id": "KhEVwGYLNkwZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "books = books.reset_index()\n",
        "books.columns = [\"Title\", \"Text\"]\n",
        "books"
      ],
      "metadata": {
        "id": "mUxVfRoqRebA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "books['Text'] = books['Text'].apply(lambda x: x.decode('utf-8'))"
      ],
      "metadata": {
        "id": "6OWjVaqa84av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change data type to string (to support further cleaning below)\n",
        "books = books.astype(str)"
      ],
      "metadata": {
        "id": "Rh1LAZH7VzRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean Texts and Set Parameters for Sectioning\n",
        "Several basic cleaning processes are implemented: removing unwanted characters from titles, removing newline characters from texts, and removing punctuation. Parameters are also set for part(s) of text to be included in sectioning. In the SciFi Corpus project, \"START OF BOOK\" and \"END OF BOOK\" tags were added to delineate the body of each text. Code in this section removes any text outside the starting and ending parameters--e.g., title page, copyright page, other paratext. "
      ],
      "metadata": {
        "id": "-74TxYNkiGbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove .txt from titles\n",
        "books['Title'] = books['Title'].str.replace(r'.txt', ' ', regex=True) \n",
        "books"
      ],
      "metadata": {
        "id": "Dlt6Q1VJv-Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove newline characters\n",
        "books['Text'] = books['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "books['Text'] = books['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "books"
      ],
      "metadata": {
        "id": "_S2txHwRX5vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation\n",
        "books['Text'] = books['Text'].str.replace(r'[^\\w\\s]+', '', regex = True)"
      ],
      "metadata": {
        "id": "h_rDtxQq8Utc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove paratext (before and after START OF BOOK and END OF BOOK tags)\n",
        "#If texts you are working with do not have these tags, ignore this cell\n",
        "\n",
        "#Split book on start of book tag, keep text only after start of book tag\n",
        "start = books[\"Text\"].str.split(\"START OF BOOK\", expand = True)\n",
        "books['Text'] = start[1]\n",
        "\n",
        "#Split book on end of book tag, keep text only before of book tag\n",
        "end = books[\"Text\"].str.split(\"END OF BOOK\", expand = True)\n",
        "books['Text'] = end[0]\n",
        "books"
      ],
      "metadata": {
        "id": "Kys5CJNyswrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check that text is cleaned and sectioned\n",
        "books.iloc[0]['Text']"
      ],
      "metadata": {
        "id": "ob2j0r6YZsOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define new dataframe\n",
        "books_cleaned = books"
      ],
      "metadata": {
        "id": "Wlyjxbp_hTiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section Texts By Chapter Headings\n",
        "When working with texts with clearly delineated chapters, using chapter headings is a relatively easy way to section texts into segments of (relatively) the same size. After checking the chapter counts for each text to confirm whether sectioning by chapter is a useful procedure, this code iterates through the texts and splits them each time it encounters a new \"chapter\" heading. From here, the text from each chapter is appended to a new dataframe and denoted by book and chapter number. "
      ],
      "metadata": {
        "id": "cj4jSFppS97y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Count number of chapters in each text\n",
        "chapter_counts = books_cleaned['Text'].str.count('CHAPTER')\n",
        "\n",
        "#Append chapter counts to dataframe\n",
        "books_cleaned[\"Chapters\"] = chapter_counts\n",
        "books_cleaned"
      ],
      "metadata": {
        "id": "Smrrk7fOhYwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make new cell each time new chapter starts \n",
        "new = books_cleaned[\"Text\"].str.split(\"CHAPTER\", expand = True).set_index(books_cleaned['Title'])\n",
        "new"
      ],
      "metadata": {
        "id": "8bH_ZGrGjRHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flatten dataframe so each chapter is on own row, designated by book and chapter \n",
        "chapters_df = new.stack().reset_index()\n",
        "chapters_df.columns = [\"Book\", \"Chapter\", \"Text\"]\n",
        "chapters_df.dropna\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "pRvTfDVZq7O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chapter labels into one column\n",
        "chapters_df['Book + Chapter'] = chapters_df['Book'].astype(str) + '_Chapter_' + chapters_df['Chapter'].astype(str)\n",
        "\n",
        "#Remove individual book and chapter columns\n",
        "chapters_df.drop(columns=['Book', 'Chapter'])\n",
        "\n",
        "#Reindex so book + chapter is first column \n",
        "column_names = \"Book + Chapter\", \"Text\"\n",
        "chapters_df = chapters_df.reindex(columns=column_names)\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "AtjYB_112gee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section Texts By Chunk of N Length\n",
        "When working with texts WITHOUT discernable chapter headings--or, even if chapter headings are present but too infrequent to split texts into meaningful segments--texts can instead be sectioned by chunks of \"N\" length, where N is a variable that can be custom-set below. After checking the word counts for each text to determine what size chunks would be appropriate, this code iterates through the texts and splits them each time it counts \"N\" number of words. From here, the text from each chunk is appended to a new dataframe and denoted by book and chunk number."
      ],
      "metadata": {
        "id": "wS7KWmxq3HQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get number of words in each book (helps to determine chunk length)\n",
        "words = books_cleaned[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
        "\n",
        "#Append chapter counts to dataframe\n",
        "books_cleaned[\"Word Count\"] = words\n",
        "books_cleaned"
      ],
      "metadata": {
        "id": "yWE_NqN-CO4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Text\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "books_cleaned['Tokens'] = books_cleaned.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
        "books_cleaned"
      ],
      "metadata": {
        "id": "XxuTbkEZNKhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define chunking function\n",
        "def split(list_a, chunk_size):\n",
        "  for i in range(0, len(list_a), chunk_size):\n",
        "    yield list_a[i:i + chunk_size]\n",
        "\n",
        "#Set desired size of chunks\n",
        "chunk_size = 1000\n",
        "\n",
        "#Create new list for chunked sentences\n",
        "chunked_sentences = []\n",
        "\n",
        "#Perform chunking function on each row of tokens\n",
        "s = books_cleaned['Tokens']\n",
        "for content in s:\n",
        "  chunks = list(split(content, chunk_size))\n",
        "  #Check that text is being chunked correctly\n",
        "  print(chunks[0])\n",
        "  #Convert from tokens to sentences to add to new df\n",
        "  for chunk in chunks:\n",
        "    sentences = ' '.join(chunk)\n",
        "    chunked_sentences.append(sentences)"
      ],
      "metadata": {
        "id": "zYZTN17_uAiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new dataframe to store chunked sentences for disaggregation\n",
        "#NEEDS WORK--\"Title\" does not reflect which chunks belong to which books\n",
        "df = pd.DataFrame(chunked_sentences).T\n",
        "chunked_df = df.stack().reset_index()\n",
        "chunked_df.columns = [\"Title\",\"Chunk\",\"Text\"]\n",
        "chunked_df"
      ],
      "metadata": {
        "id": "Z3lan4X_9aS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chunk labels into one column\n",
        "chunked_df['Book + Chunk'] = chunked_df['Title'].astype(str) + ' Chunk ' + chunked_df['Chunk'].astype(str)\n",
        "\n",
        "#Remove individual book and chunk columns\n",
        "chunked_df.drop(columns=['Title', 'Chunk'])\n",
        "\n",
        "#Reindex so book + chunk is first column \n",
        "column_names = \"Book + Chunk\", \"Text\"\n",
        "chunked_df = chunked_df.reindex(columns=column_names)\n",
        "chunked_df"
      ],
      "metadata": {
        "id": "PJiT1J4F2nMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Disaggregate Texts and Download CSV Output\n",
        "Working with texts split by chapter or chunk, the final step of this process is to disaggregate the data. Disaggregation, or the breakdown of data into smaller (disordered) parts, is accomplished through the alphabetization of the words in each chapter/chunk. \n",
        "\n",
        "The resulting \"bag of words\" data can then be downloaded as csvs and used for further analysis, such as through the Topic Modeling pipeline in the Extracted Features repository: https://github.com/SF-Nexus/Extracted-Features/blob/main/Topic%20Modeling%20with%20SciFi%20Corpus.ipynb "
      ],
      "metadata": {
        "id": "Q1A_b2-H0iah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with data from texts sectioned by CHAPTER\n",
        "#Alphabetize words in each chapter string\n",
        "chapters_df['Text'] = chapters_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "KnuyMCaTuIcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download disaggregated chapters to csv\n",
        "from google.colab import files\n",
        "\n",
        "chapters_df.to_csv('chapters_bag_of_words_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chapters_bag_of_words_output.csv')"
      ],
      "metadata": {
        "id": "fqWPBvnK44ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with data from texts sectioned by CHUNK of N length\n",
        "#Alphabetize words in each chunk string\n",
        "chunked_df['Text'] = chunked_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chunked_df"
      ],
      "metadata": {
        "id": "VfFCzNAO0kNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download disaggregated chunks to csv\n",
        "from google.colab import files\n",
        "\n",
        "chunked_df.to_csv('word_chunks_bag_of_words_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('word_chunks_bag_of_words_output.csv')"
      ],
      "metadata": {
        "id": "7oZtM8aS2DBj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
